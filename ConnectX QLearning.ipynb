{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install kaggle-environments"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.4 cannot be found (uncomment if needed). \n# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.4\n    !pip install 'kaggle-environments>=0.1.4'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create ConnectX Environment"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make\n\nenv = make(\"connectx\", debug=True)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy (more may be added later). "},{"metadata":{},"cell_type":"markdown","source":"# Critic Actor Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install 'keras'\n    \nfrom keras import backend as K\nfrom keras.layers import Activation, Dense, Input\nfrom keras.models import Model\nfrom keras.optimizers import Admas\nimport numpy as np\n    \nclass Agent_Critic_Actor(object):\n    def __init__(self, alpha, beta, gamma=0.99, n_actions=4,\n               layer1_size=1024, layer2_size=512, input_dims=8):\n        self.gamma = gamma\n        self.alpha = alpha\n        self.beta = beta\n        self.input_dims = input_dims\n        self.fc1_dims = layer1_size\n        self.fc2_dims = layer2_size\n        self.n_actions = n_actions\n            \n            \n        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n        self.actionspace = [i for i in range(self.n_actions)]\n        \n    def build_actor_critic_network(self):\n        input = Input(shape=(self.input_dims,))\n        delta = Input(shape=[1])\n        dense1 = Dense(self.fc1_dims, activation='relu')(input)\n        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n        probs = Dense(self.n_actions, activation='softmax')(dense2)\n        values = Dense(1, activaion='linear')(dense2)\n            \n        def custom_loss(y_true, y_pred):\n            out = K.clip(y_pred, 1e-8, 1- 1e-8)\n            log_lik = y_true*K.log(out)\n                \n            return K.sum(-log_lik*delta)\n            \n        actor = Model(input=[input,delta], output=[probs])\n        actor.compile(optimizer=Adam(lr=self.alpha), loss = custom_losss)\n        critic = Model(input=[input], output=[values])\n        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')\n        policy = Model(input=['input'], output=[probs])\n        return actor, critic, policy\n        \n    def choose_action(self, observation):\n        state = observation[np.newaxis, :]\n        probabilities = self.policy.predict(state)[0]\n        action = np.random.choice(self.action_space, p = probabilities)\n        return action\n        \n    def learn(self, state, action, reward, state_, done):\n        state = state[np.newaxis, :]\n        state_ = state_[np.newaxis, :]\n            \n        critic_value_ = self.critic.predic(state_)\n        critic_value = self.critic.predict(state)\n            \n        target = reward + self.gamma*critic_value_*(1-int(done))\n        delta = target - critic_value\n            \n        actions = np.zeros([1, self.n_actions])\n        actions[np.arange(1), action] = 1.0\n            \n        self.actor.fit([state,delta], actions, verbose=0)\n        self.critic.fit(state, target, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Q Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import choice\nclass QLearn:\n    \"\"\"\n        params:\n            config: Information about the board\n            gamma: importance of the value of the next state. [0,1]\n    \"\"\"\n    def __init__(self, max_num_actions, gamma = 0.9):\n        self.max_num_actions = max_num_actions\n        self.gamma = gamma\n        self.Qtable = {}\n        self.batch = []\n    \n    def state_to_str(self,state):\n        \n        def integer_to_character(elem):\n            if elem == 0:\n                return 'a'\n            elif elem == 1:\n                return 'b'\n            elif elem == 2:\n                return 'c'\n            else:\n                return 'd'\n        return ''.join(map(integer_to_character, state)) \n\n    \n    \"\"\"\n        return:\n            Chooses an action given a state that maximizes the reward\n    \"\"\"\n    def choose_action(self, state_as_list):\n        \n        state = self.state_to_str(state_as_list)\n        # If it is the first time create a space for this state\n        # and choose a random action\n        \n        if state not in self.Qtable:\n            self.Qtable[state] = [0] * self.max_num_actions\n            actions = [c for c in state_as_list if state_as_list[c] == 0]\n            if len(actions):\n                return choice(actions)\n            return 1 # Game over\n        \n        # If not return the action that maximizes the reward\n        return self.Qtable[state].index(max(self.Qtable[state]))\n    \n    \"\"\"\n        Stores the necessary parameters to update the q table.\n    \"\"\"\n    def store(self, reward, state, action, next_state):\n        self.batch.append({\"reward\":reward, \"state\":state, \"action\":action, \"next_state\":next_state})\n        \n    \"\"\"\n        param:\n            state: a list that represents the board\n        return: \n            posible actions given a state\n    \"\"\"\n    def get_actions(self, state):\n        return [c for c in self.max_num_actions if state[c] == 0]\n    \n    \"\"\"\n        Updates the q table. The value of state-action is the reward plus\n        a portion of the value of the next state. The value of the next state\n        is the max value you can get in that state\n    \"\"\"\n    def update_qtable(self, state_reward, state_as_list, action, next_state_as_list):\n        state = self.state_to_str(state_as_list)\n        next_state = self.state_to_str(next_state_as_list)\n        next_state_value = 0\n        if next_state in self.Qtable:\n            next_state_value = max(self.Qtable[next_state])\n        reward = 0\n        if state_reward is not None:\n            reward = state_reward\n        \n        self.Qtable[state][action] = reward + self.gamma * next_state_value\n    \"\"\"\n        This function is used when an episode terminates. Updates the qtable for\n        every action that the agent has made.\n    \"\"\"\n    def learn(self):\n        # Updates the q table \n        for x in self.batch:\n            self.update_qtable(x[\"reward\"], x[\"state\"], x[\"action\"], x[\"next_state\"])\n        # Restart batch \n        self.batch = []\n        \n        \n    \"\"\" UTILITIES\"\"\"\n    def get_qtable_len(self):\n        return len(self.Qtable)\n            \n \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Q Learning Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"qLearner = QLearn(env.configuration.columns)\n\ndef qlearner_choose_action(observation, configuration):\n    return qLearner.choose_action(observation)\n\nenv.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([qlearner_choose_action, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nrewards = []\nqtablelens = []\nqLearner = QLearn(env.configuration.columns)\nactions = []\n# Play as first position against random agent.\nMAX_EPISODES = 10000\nfor episode in range(MAX_EPISODES):\n    trainer = env.train([None, \"random\"])\n    state = trainer.reset()\n    total_reward = 0\n    while not env.done:\n        action = qLearner.choose_action(state.board)\n        actions.append(action)\n        next_state, reward, done, info = trainer.step(action)\n        qLearner.store(reward, state.board, action, next_state.board)\n        state = next_state\n        \n        r = -5\n        if reward is not None:\n            r = reward\n        total_reward = total_reward + r\n    qLearner.learn()\n    qtablelens.append(qLearner.get_qtable_len())\n    rewards.append(total_reward)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# fixed bin size\nbins = np.arange(-100, 100, 5) # fixed bin size\nplt.xlim([min(actions)-5, max(actions)+5])\nplt.hist(actions, bins=bins, alpha=0.5)\nplt.title('Actions distribution')\nplt.xlabel('actions')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_rewards = []\nindex = 1\ntotal_rewards = 0\nfor reward in rewards:\n    total_rewards = total_rewards + reward\n    mean_reward = total_rewards / index\n    mean_rewards.append(mean_reward)\n    index = index + 1\nplt.plot(mean_rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(qtablelens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_qtablelens = []\nindex = 1\ntotal_qtablelens = 0\nfor qtablelen in qtablelens:\n    total_qtablelens = total_qtablelens + qtablelen\n    mean_qtablelen = total_qtablelens / index\n    mean_qtablelens.append(mean_qtablelen)\n    index = index + 1\nplt.plot(mean_qtablelens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qlearner_choose_action(observation, configuration):\n    return qLearner.choose_action(observation)\nenv.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([qlearner_choose_action, \"negamax\"])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Q Learning"},{"metadata":{},"cell_type":"markdown","source":"Using Tensorflow to facilitate DQN building"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install 'keras'\n!pip install 'progressbar'\n    \nfrom keras import backend as K\nfrom keras.layers import Activation, Dense, Input\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import choice\nclass DQN:\n    def __init__(self, num_input, num_output, alpha = 0.0001):\n        self.num_input = num_input\n        self.num_output = num_output\n        self.alpha = alpha\n        self.nn = self.build_neural_network()\n        self.batch = []\n        \n    def build_neural_network(self):\n        \"\"\"\n        input = Input(shape=(self.num_input,))\n        dense1 = Dense(50, activation='relu')(input)\n        dense2 = Dense(50, activation='relu')(dense1)\n        output = Dense(self.num_output, activation='relu')(dense2)\n        nn = Model(input=input, output=output)\n        nn.compile(optimizer=Adam(lr=self.alpha), loss = 'mean_squared_error')\n        \"\"\"\n        \n        nn = Sequential()\n        nn.add(Dense(100, input_dim=self.num_input, activation='relu'))\n\n        nn.add(Dense(self.num_output, input_dim=100, activation='relu'))\n        nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n        \n        return nn\n    \n    def prepare_data(self, state):\n        return np.array([np.asarray(state.board)])\n    \n    def prepare_target(self, target):\n        return np.array([target])\n    \n    def predict(self, state):\n        return self.nn.predict(self.prepare_data(state))[0].tolist()\n    \n    def get_max_valid_action(self, state, prediction):\n        max_predicted_value = -100\n        max_action = choice([c for c in range(self.num_output) if state.board[c] == 0])\n        action = 0\n        for predicted_value in prediction:\n            if predicted_value > max_predicted_value and state.board[action] == 0:\n                max_action = action\n                max_predicted_value = predicted_value\n            action = action + 1\n        return max_action\n    \n    \"\"\"\n        Chooses an action that maximizes the agents reward\n    \"\"\"\n    def choose_action(self, state):\n        action_values = self.nn.predict(self.prepare_data(state))[0].tolist()\n        return action_values.index(max(action_values))\n    \n    \n    \"\"\"\n        Stores the necessary parameters to update the q table.\n    \"\"\"\n    def store(self, reward, state, prediction, action, next_state):\n        self.batch.append({\"reward\":reward, \"state\":state, \"prediction\":prediction, \"action\":action, \"next_state\":next_state})\n        \n    \"\"\"\n        Performs backpropagation\n    \"\"\"\n    def backprop(self, reward, state, prediction, action , next_state):\n        data = self.prepare_data(state)\n        target = np.asarray(prediction)\n        target[action] = reward\n        target = self.prepare_target(target)\n        self.nn.fit(data, target, verbose=0)\n        \n    \"\"\"\n        This function is used when an episode terminates. Updates the qtable for\n        every action that the agent has made.\n    \"\"\"\n    def learn(self):\n        # Updates the q table \n        for x in self.batch:\n            self.backprop(x[\"reward\"], x[\"state\"],x[\"prediction\"], x[\"action\"], x[\"next_state\"])\n        # Restart batch \n        self.batch = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = env.configuration.columns\nrows = env.configuration.rows\ndqn = DQN(cols * rows, cols)\n\ndef dqn_choose_action(observation, configuration):\n    return dqn.choose_action(observation)\n\nenv.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([dqn_choose_action, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom progressbar import ProgressBar\npbar = ProgressBar()\n\nrewards = []\ncols = env.configuration.columns\nrows = env.configuration.rows\ndqn = DQN(cols * rows, cols)\nactions = []\n# Play as first position against random agent.\nMAX_EPISODES = 100000\nfor episode in pbar(range(MAX_EPISODES)):\n    trainer = env.train([None, \"random\"])\n    state = trainer.reset()\n    total_reward = 0\n    while not env.done:\n        prediction = dqn.predict(state)\n        action = dqn.get_max_valid_action(state, prediction)\n        actions.append(action)\n        next_state, reward, done, info = trainer.step(action)\n        dqn.store(reward, state, prediction, action, next_state)\n        state = next_state\n        \n        r = -5\n        if reward is not None:\n            r = reward\n        total_reward = total_reward + r\n    dqn.learn()\n    rewards.append(total_reward)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# fixed bin size\nbins = np.arange(-100, 100, 5) # fixed bin size\nplt.xlim([min(actions)-5, max(actions)+5])\nplt.hist(actions, bins=bins, alpha=0.5)\nplt.title('Actions distribution')\nplt.xlabel('actions')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_rewards = []\nindex = 1\ntotal_rewards = 0\nfor reward in rewards:\n    total_rewards = total_rewards + reward\n    mean_reward = total_rewards / index\n    mean_rewards.append(mean_reward)\n    index = index + 1\nplt.plot(mean_rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This agent random chooses a non-empty column.\ndef my_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test your Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debug/Train your Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"Config:     \", str(env.configuration))\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    \n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate your Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def qlearner_choose_action(observation, configuration):\n    return qLearner.choose_action(observation)\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards if r[0] is not None) / sum(r[0] + r[1] for r in rewards if r[0] is not None)\n\n# Run multiple episodes to estimate it's performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [qlearner_choose_action, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [qlearner_choose_action, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write Submission File\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}